# ğŸ•·ï¸ ç¶²ç«™å…§å®¹çˆ¬èŸ²å·¥å…· - Agent ä½¿ç”¨æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

é€™æ˜¯ä¸€å€‹æ¨™æº–åŒ–çš„ç¶²ç«™çˆ¬èŸ²å·¥å…·ï¼Œç”¨æ–¼å®Œæ•´å‚™ä»½ç¶²ç«™å…§å®¹ã€‚

**ç•¶ä½ éœ€è¦åŸ·è¡Œä»¥ä¸‹ä»»å‹™æ™‚ï¼Œè«‹ä½¿ç”¨æ­¤å·¥å…·ï¼š**
- å‚™ä»½æ•´å€‹ç¶²ç«™å…§å®¹
- å°‡ç¶²ç«™é é¢è½‰æ›ç‚º Markdown
- ä¸‹è¼‰ç¶²ç«™çš„ä¸»è¦åœ–ç‰‡
- ç”¢ç”Ÿ SEO / Schema.org / Open Graph è¨­å®š
- ç¶²ç«™é·ç§»å‰çš„å…§å®¹æ“·å–

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### å®‰è£ä¾è³´

```bash
cd /path/to/web-crawler
pip install -r requirements.txt
```

### åŸºæœ¬ç”¨æ³•

```bash
# äº’å‹•æ¨¡å¼ï¼ˆæœƒè©¢å•ç¶²å€ï¼‰
python crawler.py

# ç›´æ¥æŒ‡å®šç¶²å€
python crawler.py https://example.com

# æŒ‡å®šè¼¸å‡ºç›®éŒ„
python crawler.py https://example.com -o ./my-backup
```

---

## ğŸ“ è¼¸å‡ºçµæ§‹

åŸ·è¡Œå¾Œæœƒç”¢ç”Ÿä»¥ä¸‹çµæ§‹ï¼š

```
./crawled-site/
â””â”€â”€ example.com/
    â”œâ”€â”€ robots.txt              # åŸå§‹ robots.txt
    â”œâ”€â”€ crawl-report.yml        # çˆ¬å–å ±å‘Š
    â”œâ”€â”€ index.md                # é¦–é  Markdown
    â”œâ”€â”€ index.yml               # é¦–é  SEO è¨­å®š
    â”œâ”€â”€ images/                 # é¦–é åœ–ç‰‡
    â”‚   â”œâ”€â”€ hero.jpg
    â”‚   â””â”€â”€ hero.yml            # åœ–ç‰‡æè¿°
    â”œâ”€â”€ about/
    â”‚   â”œâ”€â”€ index.md
    â”‚   â”œâ”€â”€ index.yml
    â”‚   â””â”€â”€ images/
    â””â”€â”€ blog/
        â””â”€â”€ post-title/
            â”œâ”€â”€ index.md
            â”œâ”€â”€ index.yml
            â””â”€â”€ images/
```

---

## âš™ï¸ åƒæ•¸èªªæ˜

| åƒæ•¸ | ç°¡å¯« | é è¨­å€¼ | èªªæ˜ |
|------|------|--------|------|
| `url` | - | ç„¡ | ç›®æ¨™ç¶²ç«™ç¶²å€ |
| `--output` | `-o` | `./crawled-site` | è¼¸å‡ºç›®éŒ„ |
| `--delay` | `-d` | `2.0` | è«‹æ±‚é–“éš”ï¼ˆç§’ï¼‰ |
| `--timeout` | `-t` | `30` | è«‹æ±‚è¶…æ™‚ï¼ˆç§’ï¼‰ |
| `--retries` | `-r` | `3` | æœ€å¤§é‡è©¦æ¬¡æ•¸ |
| `--min-image-size` | - | `100` | æœ€å°åœ–ç‰‡å°ºå¯¸ï¼ˆpxï¼‰ |

---

## ğŸ“„ è¼¸å‡ºæª”æ¡ˆèªªæ˜

### index.mdï¼ˆé é¢ Markdownï¼‰

```markdown
---
source_url: "https://example.com/about"
crawled_at: "2024-01-15T10:30:00+08:00"
---

# é—œæ–¼æˆ‘å€‘

é é¢å…§å®¹...

![åœ–ç‰‡èªªæ˜](./images/photo.jpg)
```

### index.ymlï¼ˆSEO è¨­å®šï¼‰

åŒ…å«ï¼š
- **SEO è¨­å®š**ï¼šmeta titleã€descriptionã€keywords
- **URL è¨­å®š**ï¼šè·¯å¾‘ã€slugã€é‡å°å‘è¦å‰‡
- **Schema.org**ï¼šçµæ§‹åŒ–è³‡æ–™ï¼ˆWebPageã€BreadcrumbListï¼‰
- **Open Graph**ï¼šç¤¾ç¾¤åˆ†äº«è¨­å®š
- **Twitter Card**ï¼šTwitter åˆ†äº«è¨­å®š

### {image}.ymlï¼ˆåœ–ç‰‡æè¿°ï¼‰

åŒ…å«ï¼š
- åŸºæœ¬è³‡è¨Šï¼šæª”åã€æ ¼å¼ã€å°ºå¯¸
- æè¿°ï¼šalt æ–‡å­—ã€è©³ç´°æè¿°ï¼ˆå¾… AI å¡«å…¥ï¼‰
- SEOï¼šå»ºè­°æª”åã€é—œéµå­—
- ä¾†æºï¼šåŸå§‹ URLã€æ‰€å±¬é é¢

---

## ğŸ”§ é€²éšç”¨æ³•

### åœ¨ Python ä¸­å‘¼å«

```python
from crawler import WebsiteCrawler, CrawlerConfig

# è‡ªè¨‚è¨­å®š
config = CrawlerConfig(
    output_dir='./backup',
    crawl_delay=3.0,
    min_image_size=150
)

# åŸ·è¡Œçˆ¬èŸ²
crawler = WebsiteCrawler(config)
report = crawler.crawl('https://example.com')

# æŸ¥çœ‹å ±å‘Š
print(f"æˆåŠŸçˆ¬å– {report['crawl_report']['pages']['successfully_crawled']} é ")
```

### è‡ªè¨‚æ’é™¤è¦å‰‡

ä¿®æ”¹ `CrawlerConfig` ä¸­çš„ï¼š
- `excluded_image_patterns`ï¼šæ’é™¤çš„åœ–ç‰‡ URL æ¨¡å¼
- `excluded_elements`ï¼šæ’é™¤çš„ HTML å…ƒç´ 
- `excluded_classes`ï¼šæ’é™¤çš„ CSS class é—œéµå­—

---

## âš ï¸ æ³¨æ„äº‹é …

1. **éµå®ˆ robots.txt**ï¼šå·¥å…·æœƒä¸‹è¼‰ä¸¦é¡¯ç¤º robots.txtï¼Œè«‹ç¢ºèªç›®æ¨™ç¶²ç«™å…è¨±çˆ¬å–
2. **è«‹æ±‚é–“éš”**ï¼šé è¨­ 2 ç§’ï¼Œè«‹å‹¿è¨­å¤ªçŸ­ä»¥å…é€ æˆç›®æ¨™ä¼ºæœå™¨è² æ“”
3. **ç‰ˆæ¬Šå•é¡Œ**ï¼šä¸‹è¼‰çš„å…§å®¹å¯èƒ½æœ‰ç‰ˆæ¬Šï¼Œè«‹ç¢ºèªä½¿ç”¨ç›®çš„åˆæ³•
4. **åœ–ç‰‡æè¿°**ï¼š`description.detailed` æ¬„ä½æ¨™è¨˜ç‚ºã€Œå¾… AI åˆ†æã€ï¼Œéœ€è¦å¦å¤–ä½¿ç”¨ Vision API å¡«å…¥

---

## ğŸ› å¸¸è¦‹å•é¡Œ

### Q: ç‚ºä»€éº¼æœ‰äº›é é¢æ²’æœ‰è¢«çˆ¬å–ï¼Ÿ

A: å¯èƒ½åŸå› ï¼š
- é é¢ä¸åœ¨ sitemap.xml ä¸­
- é é¢æ²’æœ‰å¾é¦–é é€£çµ
- é é¢éœ€è¦ JavaScript æ¸²æŸ“ï¼ˆæ­¤å·¥å…·ä¸æ”¯æ´ SPAï¼‰

### Q: åœ–ç‰‡ç‚ºä»€éº¼è¢«æ’é™¤ï¼Ÿ

A: åœ–ç‰‡æœƒè¢«æ’é™¤å¦‚æœï¼š
- URL åŒ…å« logoã€iconã€sprite ç­‰é—œéµå­—
- å°ºå¯¸å°æ–¼ 100x100 px
- æ˜¯é‡è¤‡çš„åœ–ç‰‡ï¼ˆåŒ hashï¼‰

### Q: å¦‚ä½•è™•ç†éœ€è¦ç™»å…¥çš„ç¶²ç«™ï¼Ÿ

A: ç›®å‰ç‰ˆæœ¬ä¸æ”¯æ´ç™»å…¥é©—è­‰ï¼Œæœªä¾†å¯æ“´å…… session/cookie åŠŸèƒ½ã€‚

---

## ğŸ“Š çˆ¬å–å ±å‘Šç¯„ä¾‹

åŸ·è¡Œå®Œæˆå¾Œæœƒç”¢ç”Ÿ `crawl-report.yml`ï¼š

```yaml
crawl_report:
  target_domain: example.com
  crawl_completed: '2024-01-15T11:30:00+08:00'
  pages:
    total_found: 50
    successfully_crawled: 48
    failed: 2
    failed_urls:
      - url: https://example.com/broken
        error: 404 Not Found
  images:
    total_found: 200
    downloaded: 150
    excluded: 40
    duplicates: 10
  files_generated:
    markdown_files: 48
    yaml_config_files: 48
    image_description_files: 150
```

---

## ğŸ”„ ç‰ˆæœ¬ç´€éŒ„

### v1.0.0
- åˆå§‹ç‰ˆæœ¬
- æ”¯æ´ sitemap.xml è§£æ
- æ”¯æ´é é¢å…§å®¹æ¸…ç†èˆ‡ Markdown è½‰æ›
- æ”¯æ´åœ–ç‰‡ä¸‹è¼‰èˆ‡å»é‡
- ç”¢ç”Ÿ SEO/Schema.org/OG YAML è¨­å®š
