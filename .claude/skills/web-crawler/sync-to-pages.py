#!/usr/bin/env python3
"""
çˆ¬èŸ²çµæœåŒæ­¥å·¥å…· (Crawl Result Sync Tool)

ç”¨é€”ï¼š
    å°‡çˆ¬èŸ²è¼¸å‡º (crawl-temp/) æ™ºæ…§åˆä½µåˆ° pages/ ç›®éŒ„ã€‚

åŠŸèƒ½ï¼š
    1. è­˜åˆ¥ placeholder é é¢ vs å·²å„ªåŒ–é é¢
    2. æ™ºæ…§åˆä½µç­–ç•¥ï¼š
       - placeholder é é¢ï¼šè¤‡è£½ index.md å’Œåœ–ç‰‡
       - å·²å„ªåŒ–é é¢ï¼šå¯é¸æ“‡åªæ›´æ–° index.md
    3. è½‰æ›åœ–ç‰‡ URLï¼ˆçµ•å°è·¯å¾‘ â†’ ç›¸å°è·¯å¾‘ï¼‰
    4. é‡å‘½å images/ â†’ assets/
    5. ç”¢ç”Ÿåœ–ç‰‡ ID æ˜ å°„å»ºè­°

ä½¿ç”¨æ–¹å¼ï¼š
    # é è¦½æ¨¡å¼ï¼ˆé è¨­ï¼‰ï¼šé¡¯ç¤ºå°‡åŸ·è¡Œçš„æ“ä½œ
    python sync-to-pages.py

    # åŸ·è¡ŒåŒæ­¥
    python sync-to-pages.py --execute

    # åªè™•ç†ç‰¹å®šé é¢
    python sync-to-pages.py --pages services,contact --execute

    # å¼·åˆ¶è¦†è“‹å·²å„ªåŒ–é é¢
    python sync-to-pages.py --force --execute

Author: ewill-web Agent
Version: 1.0.0
"""

import os
import re
import sys
import shutil
import argparse
from pathlib import Path
from typing import List, Dict, Set, Tuple
from urllib.parse import urlparse, unquote

import yaml


# ============================================
# è¨­å®šå€ (Configuration)
# ============================================

# å°ˆæ¡ˆè·¯å¾‘
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
CRAWL_TEMP_DIR = PROJECT_ROOT / "crawl-temp"
PAGES_DIR = PROJECT_ROOT / "pages"

# æ’é™¤çš„ç›®éŒ„ï¼ˆä¸åŒæ­¥ï¼‰
EXCLUDED_DIRS = {
    'images',           # å…±ç”¨åœ–ç‰‡ç›®éŒ„
    'category',         # WordPress åˆ†é¡é 
    'elementor-hf',     # Elementor å…§éƒ¨é é¢
    'hello-world',      # æ¸¬è©¦é é¢
    'solutions_old2',   # èˆŠç‰ˆé é¢
}

# placeholder é é¢è­˜åˆ¥é—œéµå­—
PLACEHOLDER_KEYWORDS = ['å»ºç½®ä¸­', 'æ•¬è«‹æœŸå¾…', 'Coming Soon']


# ============================================
# å·¥å…·å‡½æ•¸
# ============================================

def is_placeholder_page(pages_dir: Path, page_name: str) -> bool:
    """æª¢æŸ¥é é¢æ˜¯å¦ç‚º placeholder"""
    yml_path = pages_dir / page_name / "index.yml"
    if not yml_path.exists():
        return True  # æ²’æœ‰ yml è¦–ç‚ºéœ€è¦æ›´æ–°

    try:
        with open(yml_path, 'r', encoding='utf-8') as f:
            content = f.read()
            for keyword in PLACEHOLDER_KEYWORDS:
                if keyword in content:
                    return True
    except Exception:
        pass

    return False


def get_crawled_domains(crawl_temp_dir: Path) -> List[str]:
    """å–å¾—å·²çˆ¬å–çš„ç¶²åŸŸåˆ—è¡¨"""
    domains = []
    if crawl_temp_dir.exists():
        for item in crawl_temp_dir.iterdir():
            if item.is_dir() and not item.name.startswith('.'):
                domains.append(item.name)
    return domains


def get_crawled_pages(domain_dir: Path) -> Set[str]:
    """å–å¾—çˆ¬å–çš„é é¢åˆ—è¡¨"""
    pages = set()
    for item in domain_dir.iterdir():
        if item.is_dir() and item.name not in EXCLUDED_DIRS:
            pages.add(item.name)
    return pages


def get_existing_pages(pages_dir: Path) -> Set[str]:
    """å–å¾—ç¾æœ‰é é¢åˆ—è¡¨"""
    pages = set()
    for item in pages_dir.iterdir():
        if item.is_dir() and not item.name.startswith('.'):
            pages.add(item.name)
    return pages


def convert_image_urls(content: str, page_name: str) -> Tuple[str, List[str]]:
    """
    è½‰æ› Markdown ä¸­çš„åœ–ç‰‡ URL

    çµ•å° URL â†’ ç›¸å°è·¯å¾‘ (./assets/filename.ext)

    Returns:
        (converted_content, list_of_image_filenames)
    """
    images_found = []

    def replace_url(match):
        alt_text = match.group(1)
        url = match.group(2)

        # è§£æ URL
        parsed = urlparse(url)

        # å¦‚æœæ˜¯çµ•å° URLï¼Œæå–æª”å
        if parsed.scheme in ('http', 'https'):
            filename = Path(unquote(parsed.path)).name
            images_found.append(filename)
            return f'![{alt_text}](./assets/{filename})'

        # å·²ç¶“æ˜¯ç›¸å°è·¯å¾‘
        if url.startswith('./'):
            filename = Path(url).name
            images_found.append(filename)
            return match.group(0)

        return match.group(0)

    # åŒ¹é… Markdown åœ–ç‰‡èªæ³• ![alt](url)
    pattern = r'!\[([^\]]*)\]\(([^)]+)\)'
    converted = re.sub(pattern, replace_url, content)

    return converted, images_found


def sync_page(
    crawl_dir: Path,
    pages_dir: Path,
    page_name: str,
    force: bool = False,
    execute: bool = False
) -> Dict:
    """
    åŒæ­¥å–®ä¸€é é¢

    Returns:
        æ“ä½œçµæœå ±å‘Š
    """
    result = {
        'page': page_name,
        'status': 'skipped',
        'actions': [],
        'images_converted': [],
    }

    source_dir = crawl_dir / page_name
    target_dir = pages_dir / page_name

    # æª¢æŸ¥ä¾†æºæ˜¯å¦å­˜åœ¨
    if not source_dir.exists():
        result['status'] = 'source_not_found'
        return result

    # æª¢æŸ¥æ˜¯å¦ç‚º placeholder æˆ–å¼·åˆ¶æ›´æ–°
    is_placeholder = is_placeholder_page(pages_dir, page_name)

    if not is_placeholder and not force:
        result['status'] = 'optimized_skipped'
        result['actions'].append('é é¢å·²å„ªåŒ–ï¼Œè·³éï¼ˆä½¿ç”¨ --force å¼·åˆ¶æ›´æ–°ï¼‰')
        return result

    # å»ºç«‹ç›®æ¨™ç›®éŒ„
    if execute:
        target_dir.mkdir(parents=True, exist_ok=True)
        (target_dir / 'assets').mkdir(exist_ok=True)

    # 1. è™•ç† index.md
    source_md = source_dir / 'index.md'
    if source_md.exists():
        with open(source_md, 'r', encoding='utf-8') as f:
            content = f.read()

        # è½‰æ›åœ–ç‰‡ URL
        converted_content, images = convert_image_urls(content, page_name)
        result['images_converted'] = images

        if execute:
            target_md = target_dir / 'index.md'
            with open(target_md, 'w', encoding='utf-8') as f:
                f.write(converted_content)

        result['actions'].append(f'æ›´æ–° index.mdï¼ˆ{len(images)} å¼µåœ–ç‰‡ URL å·²è½‰æ›ï¼‰')

    # 2. è¤‡è£½åœ–ç‰‡
    source_images = source_dir / 'images'
    if source_images.exists():
        image_count = 0
        for img_file in source_images.iterdir():
            if img_file.is_file():
                if execute:
                    target_img = target_dir / 'assets' / img_file.name
                    shutil.copy2(img_file, target_img)
                image_count += 1

        if image_count > 0:
            result['actions'].append(f'è¤‡è£½ {image_count} å€‹åœ–ç‰‡æª”æ¡ˆåˆ° assets/')

    # 3. å»ºç«‹/æ›´æ–° index.ymlï¼ˆå¦‚æœæ˜¯æ–°é é¢ï¼‰
    if not target_dir.exists() or not (target_dir / 'index.yml').exists():
        source_yml = source_dir / 'index.yml'
        if source_yml.exists():
            # å¾çˆ¬å–çš„ yml æå– SEO è³‡è¨Š
            with open(source_yml, 'r', encoding='utf-8') as f:
                crawled_yml = yaml.safe_load(f)

            # å»ºç«‹æ–°çš„ yml çµæ§‹
            new_yml = {
                'seo': crawled_yml.get('seo', {}),
                'url_mapping': {
                    'current_url': f'/{page_name}/',
                    'old_url': f'/{page_name}/',
                    'redirect': False
                },
                'layout': {
                    'sections': [
                        {'type': 'content', 'source': 'index.md'}
                    ]
                }
            }

            if execute:
                target_yml = target_dir / 'index.yml'
                with open(target_yml, 'w', encoding='utf-8') as f:
                    yaml.dump(new_yml, f, allow_unicode=True, default_flow_style=False)

            result['actions'].append('å»ºç«‹ index.yml')

    result['status'] = 'success' if result['actions'] else 'no_changes'
    return result


def generate_asset_mapping_suggestions(pages_dir: Path) -> List[Dict]:
    """
    ç”¢ç”Ÿåœ–ç‰‡ ID æ˜ å°„å»ºè­°

    æƒææ‰€æœ‰é é¢çš„ assets/ ç›®éŒ„ï¼Œå»ºè­°æ¨™æº–åŒ–çš„ ID
    """
    suggestions = []

    for page_dir in pages_dir.iterdir():
        if not page_dir.is_dir():
            continue

        assets_dir = page_dir / 'assets'
        if not assets_dir.exists():
            continue

        for img_file in assets_dir.iterdir():
            if img_file.suffix.lower() in ('.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg'):
                # å»ºè­° IDï¼šé é¢åç¨±_æª”åï¼ˆsnake_caseï¼‰
                suggested_id = f"{page_dir.name}_{img_file.stem}".lower()
                suggested_id = re.sub(r'[^a-z0-9]+', '_', suggested_id)
                suggested_id = re.sub(r'_+', '_', suggested_id).strip('_')

                suggestions.append({
                    'file': str(img_file.relative_to(pages_dir)),
                    'current_name': img_file.name,
                    'suggested_id': suggested_id
                })

    return suggestions


# ============================================
# ä¸»ç¨‹å¼
# ============================================

def main():
    parser = argparse.ArgumentParser(
        description='å°‡çˆ¬èŸ²çµæœåŒæ­¥åˆ° pages/ ç›®éŒ„',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument(
        '--execute', '-e',
        action='store_true',
        help='åŸ·è¡ŒåŒæ­¥ï¼ˆé è¨­ç‚ºé è¦½æ¨¡å¼ï¼‰'
    )

    parser.add_argument(
        '--pages', '-p',
        type=str,
        help='åªè™•ç†æŒ‡å®šé é¢ï¼ˆé€—è™Ÿåˆ†éš”ï¼‰'
    )

    parser.add_argument(
        '--force', '-f',
        action='store_true',
        help='å¼·åˆ¶æ›´æ–°å·²å„ªåŒ–é é¢'
    )

    parser.add_argument(
        '--domain', '-d',
        type=str,
        help='æŒ‡å®šç¶²åŸŸï¼ˆé è¨­ä½¿ç”¨ç¬¬ä¸€å€‹æ‰¾åˆ°çš„ï¼‰'
    )

    parser.add_argument(
        '--suggestions', '-s',
        action='store_true',
        help='ç”¢ç”Ÿåœ–ç‰‡ ID æ˜ å°„å»ºè­°'
    )

    args = parser.parse_args()

    print("=" * 60)
    print("ğŸ“¦ çˆ¬èŸ²çµæœåŒæ­¥å·¥å…· v1.0.0")
    print("=" * 60)

    # æª¢æŸ¥ç›®éŒ„
    if not CRAWL_TEMP_DIR.exists():
        print(f"âŒ æ‰¾ä¸åˆ° crawl-temp ç›®éŒ„ï¼š{CRAWL_TEMP_DIR}")
        sys.exit(1)

    if not PAGES_DIR.exists():
        print(f"âŒ æ‰¾ä¸åˆ° pages ç›®éŒ„ï¼š{PAGES_DIR}")
        sys.exit(1)

    # å–å¾—ç¶²åŸŸ
    domains = get_crawled_domains(CRAWL_TEMP_DIR)
    if not domains:
        print("âŒ crawl-temp ç›®éŒ„ä¸­æ²’æœ‰çˆ¬å–çµæœ")
        sys.exit(1)

    domain = args.domain if args.domain else domains[0]
    domain_dir = CRAWL_TEMP_DIR / domain

    if not domain_dir.exists():
        print(f"âŒ æ‰¾ä¸åˆ°ç¶²åŸŸç›®éŒ„ï¼š{domain}")
        sys.exit(1)

    print(f"ğŸ“ ä¾†æºï¼š{domain_dir}")
    print(f"ğŸ“ ç›®æ¨™ï¼š{PAGES_DIR}")
    print(f"ğŸ”§ æ¨¡å¼ï¼š{'åŸ·è¡Œ' if args.execute else 'é è¦½'}")
    print("=" * 60)

    # å–å¾—é é¢åˆ—è¡¨
    crawled_pages = get_crawled_pages(domain_dir)
    existing_pages = get_existing_pages(PAGES_DIR)

    # éæ¿¾æŒ‡å®šé é¢
    if args.pages:
        target_pages = set(args.pages.split(','))
        crawled_pages = crawled_pages & target_pages

    print(f"\nğŸ“Š é é¢çµ±è¨ˆï¼š")
    print(f"   çˆ¬å–é é¢ï¼š{len(crawled_pages)}")
    print(f"   ç¾æœ‰é é¢ï¼š{len(existing_pages)}")
    print(f"   æ–°å¢é é¢ï¼š{len(crawled_pages - existing_pages)}")

    # åŒæ­¥æ¯å€‹é é¢
    results = []
    for page_name in sorted(crawled_pages):
        result = sync_page(
            domain_dir,
            PAGES_DIR,
            page_name,
            force=args.force,
            execute=args.execute
        )
        results.append(result)

    # è¼¸å‡ºçµæœ
    print(f"\nğŸ“‹ åŒæ­¥çµæœï¼š")
    print("-" * 60)

    status_counts = {}
    for result in results:
        status = result['status']
        status_counts[status] = status_counts.get(status, 0) + 1

        icon = {
            'success': 'âœ…',
            'optimized_skipped': 'â­ï¸',
            'source_not_found': 'âŒ',
            'no_changes': 'â–',
            'skipped': 'â­ï¸'
        }.get(status, 'â“')

        print(f"{icon} {result['page']}")
        for action in result['actions']:
            print(f"   â””â”€ {action}")

    print("-" * 60)
    print(f"\nğŸ“Š çµ±è¨ˆï¼š")
    for status, count in status_counts.items():
        print(f"   {status}: {count}")

    # ç”¢ç”Ÿåœ–ç‰‡æ˜ å°„å»ºè­°
    if args.suggestions:
        print(f"\nğŸ–¼ï¸ åœ–ç‰‡ ID æ˜ å°„å»ºè­°ï¼š")
        print("-" * 60)
        suggestions = generate_asset_mapping_suggestions(PAGES_DIR)
        for s in suggestions[:20]:  # åªé¡¯ç¤ºå‰ 20 å€‹
            print(f"   {s['current_name']} â†’ {s['suggested_id']}")
        if len(suggestions) > 20:
            print(f"   ... é‚„æœ‰ {len(suggestions) - 20} å€‹")

    if not args.execute:
        print(f"\nğŸ’¡ é€™æ˜¯é è¦½æ¨¡å¼ã€‚ä½¿ç”¨ --execute åŸ·è¡Œå¯¦éš›åŒæ­¥ã€‚")


if __name__ == '__main__':
    main()
