#!/usr/bin/env python3
"""
ç¶²ç«™å…§å®¹çˆ¬èŸ²èˆ‡å‚™ä»½å·¥å…· (Website Content Crawler & Backup Tool)

ç”¨é€”ï¼š
    å®Œæ•´å‚™ä»½ä¸€å€‹ç¶²ç«™çš„å…§å®¹ï¼ŒåŒ…å«é é¢ã€åœ–ç‰‡ã€SEO è¨­å®šç­‰ã€‚

ä½¿ç”¨æ–¹å¼ï¼š
    # åˆ†éšæ®µæ¨¡å¼ï¼ˆé è¨­ï¼‰ï¼šå…ˆçˆ¬é¦–é ç¢ºèªï¼Œå†çˆ¬å…¨ç«™
    python crawler.py https://example.com
    
    # é è¦½æ¨¡å¼ï¼šåªçˆ¬é¦–é ï¼Œç”¢ç”Ÿé è¦½ä¾›ç¢ºèª
    python crawler.py https://example.com --preview
    
    # ç¹¼çºŒçˆ¬å–ï¼šç¢ºèªç„¡èª¤å¾Œç¹¼çºŒçˆ¬å–å…¨ç«™
    python crawler.py https://example.com --continue
    
    # ä¸€æ¬¡çˆ¬å®Œï¼ˆè·³éç¢ºèªï¼‰
    python crawler.py https://example.com --full
    
    # Debug æ¨¡å¼ï¼šé¡¯ç¤ºè©³ç´°çš„åœ–ç‰‡æ“·å–éç¨‹
    python crawler.py https://example.com --debug

è¼¸å‡ºï¼š
    ./{domain}/
    â”œâ”€â”€ index.md                 # é é¢ Markdown
    â”œâ”€â”€ index.yml                # SEO/Schema.org è¨­å®š
    â”œâ”€â”€ images/                  # é é¢åœ–ç‰‡
    â”‚   â”œâ”€â”€ image-name.jpg
    â”‚   â””â”€â”€ image-name.yml       # åœ–ç‰‡æè¿°
    â””â”€â”€ ...

Author: Auto-generated for Agent use
Version: 1.2.0 (ä¿®æ­£åœ–ç‰‡æ“·å–å•é¡Œ)
"""

import os
import re
import sys
import json
import time
import hashlib
import argparse
import mimetypes
from pathlib import Path
from datetime import datetime
from urllib.parse import urlparse, urljoin, unquote
from typing import Optional
from dataclasses import dataclass, field

import requests
from bs4 import BeautifulSoup
import yaml


# ============================================
# è¨­å®šå€ (Configuration)
# ============================================

@dataclass
class CrawlerConfig:
    """çˆ¬èŸ²è¨­å®š"""
    # åŸ·è¡Œæ¨¡å¼
    mode: str = 'preview'  # preview, continue, full
    
    # Debug æ¨¡å¼
    debug: bool = False
    
    # è«‹æ±‚è¨­å®š
    crawl_delay: float = 2.0  # è«‹æ±‚é–“éš”ï¼ˆç§’ï¼‰
    timeout: int = 30  # è«‹æ±‚è¶…æ™‚ï¼ˆç§’ï¼‰
    max_retries: int = 3  # æœ€å¤§é‡è©¦æ¬¡æ•¸
    
    # User-Agent
    user_agent: str = "Mozilla/5.0 (compatible; ContentBackupBot/1.0)"
    
    # è¼¸å‡ºè¨­å®š
    output_dir: str = "../../../"  # ä¿®æ­£ï¼šè¼¸å‡ºåˆ°å°ˆæ¡ˆæ ¹ç›®éŒ„
    
    # åœ–ç‰‡éæ¿¾è¦å‰‡ï¼ˆåªæ’é™¤æ˜ç¢ºçš„è£é£¾åœ–ç‰‡å’Œå»£å‘Šï¼‰
    excluded_image_patterns: list = field(default_factory=lambda: [
        r'^favicon', r'sprite',
        r'1x1\.', r'tracking', r'analytics',
        r'advertisement', r'banner[-_]ad',
        r'pixel\.gif', r'spacer\.gif'
    ])
    
    # æœ€å°åœ–ç‰‡å°ºå¯¸ï¼ˆå°æ–¼æ­¤å°ºå¯¸è¦–ç‚ºè£é£¾åœ–ç‰‡ï¼‰
    min_image_size: int = 50  # pxï¼ˆé™ä½é–€æª»ï¼‰
    
    # æ’é™¤çš„ HTML å…ƒç´ 
    excluded_elements: list = field(default_factory=lambda: [
        'script', 'style', 'noscript', 'iframe', 
        'form', 'button', 'input', 'select', 'textarea'
    ])
    
    # æ’é™¤çš„ class é—œéµå­—ï¼ˆä½¿ç”¨ç²¾ç¢ºåŒ¹é…ï¼‰
    excluded_classes: list = field(default_factory=lambda: [
        'advertisement', 'cookie-banner', 'popup', 'modal',
        'social-share', 'comment-form'
    ])
    
    # æ’é™¤çš„ id é—œéµå­—
    excluded_ids: list = field(default_factory=lambda: [
        'cookie-notice', 'popup-overlay', 'ad-container'
    ])

    # æ’é™¤çš„ URL è·¯å¾‘ï¼ˆç”¨æ–¼éæ¿¾ä¸éœ€è¦çˆ¬å–çš„é é¢ï¼‰
    excluded_paths: list = field(default_factory=list)

    # åœ–ç‰‡ç›®éŒ„åç¨±ï¼ˆdefault: 'images', pages æ ¼å¼: 'assets'ï¼‰
    images_dir_name: str = 'images'


# ============================================
# å·¥å…·å‡½å¼ (Utility Functions)
# ============================================

def slugify(text: str) -> str:
    """å°‡æ–‡å­—è½‰æ›ç‚º URL-friendly çš„ slug"""
    text = text.lower().strip()
    text = re.sub(r'[^\w\s-]', '', text)
    text = re.sub(r'[-\s]+', '-', text)
    return text.strip('-')


def normalize_url(url: str) -> str:
    """
    æ­£è¦åŒ– URLï¼Œç¢ºä¿ç›¸åŒé é¢åªæœ‰ä¸€å€‹ URL è¡¨ç¤º

    - ç§»é™¤çµå°¾æ–œç·šï¼ˆé¦–é é™¤å¤–ï¼‰
    - ç§»é™¤ query string å’Œ fragment

    Examples:
        /about_us/ â†’ /about_us
        /about_us  â†’ /about_us
        /          â†’ /
    """
    parsed = urlparse(url)
    path = parsed.path

    # ç§»é™¤çµå°¾æ–œç·šï¼ˆä½†ä¿ç•™æ ¹è·¯å¾‘ /ï¼‰
    if path != '/' and path.endswith('/'):
        path = path.rstrip('/')

    # é‡å»º URLï¼ˆä¸å« query string å’Œ fragmentï¼‰
    return f"{parsed.scheme}://{parsed.netloc}{path}"


def url_to_path(url: str, base_domain: str) -> Path:
    """å°‡ URL è½‰æ›ç‚ºæª”æ¡ˆç³»çµ±è·¯å¾‘"""
    parsed = urlparse(url)
    path = parsed.path.strip('/')
    
    if not path:
        return Path(base_domain)
    
    # ç§»é™¤ .html, .htm, .php ç­‰å‰¯æª”å
    path = re.sub(r'\.(html?|php|aspx?)$', '', path, flags=re.IGNORECASE)
    
    # è½‰æ›ç‚ºå®‰å…¨çš„è·¯å¾‘
    parts = [slugify(unquote(p)) for p in path.split('/') if p]
    
    return Path(base_domain) / '/'.join(parts) if parts else Path(base_domain)


def get_file_hash(content: bytes) -> str:
    """è¨ˆç®—æª”æ¡ˆçš„ MD5 hash"""
    return hashlib.md5(content).hexdigest()


def sanitize_filename(filename: str) -> str:
    """æ¸…ç†æª”æ¡ˆåç¨±"""
    # ç§»é™¤ä¸å®‰å…¨å­—å…ƒ
    filename = re.sub(r'[<>:"/\\|?*]', '', filename)
    # é™åˆ¶é•·åº¦
    name, ext = os.path.splitext(filename)
    if len(name) > 100:
        name = name[:100]
    return f"{name}{ext}"


def format_datetime() -> str:
    """å–å¾— ISO 8601 æ ¼å¼çš„ç¾åœ¨æ™‚é–“"""
    return datetime.now().astimezone().isoformat()


# ============================================
# ç¶²é æ“·å–å™¨ (Web Fetcher)
# ============================================

class WebFetcher:
    """è² è²¬ç¶²é è«‹æ±‚çš„é¡åˆ¥"""
    
    def __init__(self, config: CrawlerConfig):
        self.config = config
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': config.user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/*,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
        })
        self.last_request_time = 0
    
    def _wait_for_delay(self):
        """ç­‰å¾…çˆ¬å–é–“éš”"""
        elapsed = time.time() - self.last_request_time
        if elapsed < self.config.crawl_delay:
            time.sleep(self.config.crawl_delay - elapsed)
    
    def fetch(self, url: str, binary: bool = False) -> Optional[bytes | str]:
        """æ“·å–ç¶²é å…§å®¹"""
        self._wait_for_delay()
        
        for attempt in range(self.config.max_retries):
            try:
                response = self.session.get(
                    url,
                    timeout=self.config.timeout,
                    allow_redirects=True
                )
                response.raise_for_status()
                self.last_request_time = time.time()
                
                if binary:
                    return response.content
                return response.text
                
            except requests.RequestException as e:
                print(f"  âš ï¸  å˜—è©¦ {attempt + 1}/{self.config.max_retries} å¤±æ•—: {e}")
                if attempt < self.config.max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•¸é€€é¿
                    
        return None
    
    def fetch_robots_txt(self, domain: str) -> Optional[str]:
        """æ“·å– robots.txt"""
        url = f"https://{domain}/robots.txt"
        return self.fetch(url)
    
    def fetch_sitemap(self, domain: str) -> list[str]:
        """æ“·å– sitemap.xml ä¸­çš„æ‰€æœ‰ URL"""
        urls = []
        sitemap_url = f"https://{domain}/sitemap.xml"
        
        content = self.fetch(sitemap_url)
        if not content:
            return urls
        
        try:
            soup = BeautifulSoup(content, 'xml')
            
            # è™•ç† sitemap index
            sitemaps = soup.find_all('sitemap')
            if sitemaps:
                for sitemap in sitemaps:
                    loc = sitemap.find('loc')
                    if loc:
                        sub_urls = self._parse_sitemap(loc.text)
                        urls.extend(sub_urls)
            else:
                urls = self._parse_sitemap_content(soup)
                
        except Exception as e:
            print(f"  âš ï¸  è§£æ sitemap å¤±æ•—: {e}")
            
        return urls
    
    def _parse_sitemap(self, url: str) -> list[str]:
        """è§£æå–®å€‹ sitemap"""
        content = self.fetch(url)
        if not content:
            return []
        
        soup = BeautifulSoup(content, 'xml')
        return self._parse_sitemap_content(soup)
    
    def _parse_sitemap_content(self, soup: BeautifulSoup) -> list[str]:
        """å¾ sitemap soup ä¸­æå– URL"""
        urls = []
        for url_tag in soup.find_all('url'):
            loc = url_tag.find('loc')
            if loc:
                urls.append(loc.text)
        return urls


# ============================================
# å…§å®¹è§£æå™¨ (Content Parser)
# ============================================

class ContentParser:
    """è² è²¬è§£æå’Œæ¸…ç† HTML å…§å®¹"""
    
    def __init__(self, config: CrawlerConfig):
        self.config = config
    
    def parse(self, html: str, base_url: str) -> dict:
        """è§£æ HTML ä¸¦æå–ä¸»è¦å…§å®¹"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # æå– metadata
        metadata = self._extract_metadata(soup)
        
        # [é‡è¦ä¿®æ­£] å…ˆå¾å®Œæ•´é é¢æå–æ‰€æœ‰åœ–ç‰‡ï¼Œå†æ¸…ç† HTML
        all_images_before_clean = self._extract_all_images_from_soup(soup, base_url)
        
        if self.config.debug:
            print(f"      [DEBUG] æ¸…ç†å‰æ‰¾åˆ° {len(all_images_before_clean)} å¼µåœ–ç‰‡")
        
        # æ¸…ç† HTML
        cleaned_soup = self._clean_html(soup)
        
        # æå–ä¸»è¦å…§å®¹
        main_content = self._extract_main_content(cleaned_soup)
        
        # å¾ä¸»è¦å…§å®¹å€å¡Šæå–åœ–ç‰‡
        main_content_images = self._extract_images(main_content, base_url)
        
        if self.config.debug:
            print(f"      [DEBUG] ä¸»å…§å®¹å€å¡Šæ‰¾åˆ° {len(main_content_images)} å¼µåœ–ç‰‡")
        
        # [é‡è¦ä¿®æ­£] åˆä½µåœ–ç‰‡ï¼šå„ªå…ˆä½¿ç”¨ä¸»å…§å®¹çš„ï¼Œä½†è£œä¸Šå¯èƒ½æ¼æ‰çš„
        images = self._merge_images(main_content_images, all_images_before_clean)
        
        if self.config.debug:
            print(f"      [DEBUG] åˆä½µå¾Œå…± {len(images)} å¼µåœ–ç‰‡")
        
        # è½‰æ›ç‚º Markdown
        markdown = self._to_markdown(main_content, base_url)
        
        return {
            'metadata': metadata,
            'markdown': markdown,
            'images': images,
            'soup': main_content
        }
    
    def _extract_metadata(self, soup: BeautifulSoup) -> dict:
        """æå–é é¢ metadata"""
        metadata = {
            'title': '',
            'description': '',
            'keywords': [],
            'og_title': '',
            'og_description': '',
            'og_image': '',
            'canonical': ''
        }
        
        # Title
        title_tag = soup.find('title')
        if title_tag:
            metadata['title'] = title_tag.get_text(strip=True)
        
        # Meta tags
        for meta in soup.find_all('meta'):
            name = meta.get('name', '').lower()
            property_attr = meta.get('property', '').lower()
            content = meta.get('content', '')
            
            if name == 'description':
                metadata['description'] = content
            elif name == 'keywords':
                metadata['keywords'] = [k.strip() for k in content.split(',')]
            elif property_attr == 'og:title':
                metadata['og_title'] = content
            elif property_attr == 'og:description':
                metadata['og_description'] = content
            elif property_attr == 'og:image':
                metadata['og_image'] = content
        
        # Canonical
        canonical = soup.find('link', rel='canonical')
        if canonical:
            metadata['canonical'] = canonical.get('href', '')
        
        return metadata
    
    def _clean_html(self, soup: BeautifulSoup) -> BeautifulSoup:
        """æ¸…ç† HTMLï¼Œç§»é™¤ä¸éœ€è¦çš„å…ƒç´ ï¼ˆæ›´ä¿å®ˆçš„ç­–ç•¥ï¼‰"""
        soup = BeautifulSoup(str(soup), 'html.parser')
        
        # åªç§»é™¤ç¢ºå®šä¸éœ€è¦çš„å…ƒç´ 
        for tag_name in self.config.excluded_elements:
            for tag in soup.find_all(tag_name):
                tag.decompose()
        
        # [ä¿®æ­£] ä½¿ç”¨ç²¾ç¢ºçš„ class åŒ¹é…ï¼ˆword boundaryï¼‰
        for class_keyword in self.config.excluded_classes:
            # åªåŒ¹é…å®Œæ•´çš„ class åç¨±
            for tag in soup.find_all(class_=lambda x: x and class_keyword in x.split()):
                if self.config.debug:
                    print(f"      [DEBUG] ç§»é™¤å…ƒç´  (class={class_keyword}): {tag.name}")
                tag.decompose()
        
        # [ä¿®æ­£] ä½¿ç”¨ç²¾ç¢ºçš„ id åŒ¹é…
        for id_keyword in self.config.excluded_ids:
            for tag in soup.find_all(id=id_keyword):
                if self.config.debug:
                    print(f"      [DEBUG] ç§»é™¤å…ƒç´  (id={id_keyword}): {tag.name}")
                tag.decompose()
        
        return soup
    
    def _extract_main_content(self, soup: BeautifulSoup) -> BeautifulSoup:
        """æå–ä¸»è¦å…§å®¹å€å¡Š"""
        # å„ªå…ˆå°‹æ‰¾ main, article, æˆ–ç‰¹å®š class
        main_selectors = [
            'main',
            'article',
            '[role="main"]',
            '.main-content',
            '.content',
            '.post-content',
            '.entry-content',
            '.article-content',
            '#content',
            '#main'
        ]
        
        for selector in main_selectors:
            content = soup.select_one(selector)
            if content and len(content.get_text(strip=True)) > 100:
                if self.config.debug:
                    print(f"      [DEBUG] ä½¿ç”¨ä¸»å…§å®¹ selector: {selector}")
                return content
        
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œä½¿ç”¨ body
        body = soup.find('body')
        if self.config.debug:
            print(f"      [DEBUG] æ‰¾ä¸åˆ°ä¸»å…§å®¹å€å¡Šï¼Œä½¿ç”¨ body")
        return body if body else soup
    
    def _extract_all_images_from_soup(self, soup: BeautifulSoup, base_url: str) -> list[dict]:
        """å¾å®Œæ•´ soup ä¸­æå–æ‰€æœ‰åœ–ç‰‡ï¼ˆæ¸…ç†å‰ï¼‰"""
        return self._extract_images_internal(soup, base_url, is_pre_clean=True)
    
    def _extract_images(self, soup: BeautifulSoup, base_url: str) -> list[dict]:
        """æå–ä¸»è¦å…§å®¹åœ–ç‰‡"""
        return self._extract_images_internal(soup, base_url, is_pre_clean=False)
    
    def _extract_images_internal(self, soup: BeautifulSoup, base_url: str, is_pre_clean: bool = False) -> list[dict]:
        """å…§éƒ¨åœ–ç‰‡æå–é‚è¼¯"""
        images = []
        seen_urls = set()
        
        # æ”¶é›†æ‰€æœ‰å¯èƒ½åŒ…å«åœ–ç‰‡çš„å…ƒç´ 
        img_elements = []
        
        # 1. æ¨™æº– img æ¨™ç±¤
        img_elements.extend(soup.find_all('img'))
        
        # 2. picture > source æ¨™ç±¤
        for source in soup.find_all('source'):
            img_elements.append(source)
        
        # 3. å¸¶æœ‰ background-image çš„å…ƒç´ ï¼ˆå¸¸è¦‹æ–¼ lazy-loadï¼‰
        for tag in soup.find_all(attrs={'data-bg': True}):
            img_elements.append(tag)
        for tag in soup.find_all(attrs={'data-background': True}):
            img_elements.append(tag)
        
        if self.config.debug and not is_pre_clean:
            print(f"      [DEBUG] æ‰¾åˆ° {len(img_elements)} å€‹æ½›åœ¨åœ–ç‰‡å…ƒç´ ")
        
        for img in img_elements:
            # [ä¿®æ­£] å˜—è©¦æ›´å¤š lazy-load å±¬æ€§
            src = self._get_image_src(img)
            
            if not src:
                if self.config.debug and not is_pre_clean:
                    print(f"      [DEBUG] è·³éç„¡ src çš„å…ƒç´ : {img.name}")
                continue
            
            # è½‰æ›ç‚ºçµ•å° URL
            abs_url = urljoin(base_url, src)
            
            # è·³éå·²è™•ç†çš„ URL
            if abs_url in seen_urls:
                continue
            seen_urls.add(abs_url)
            
            # æª¢æŸ¥æ˜¯å¦æ‡‰è©²æ’é™¤
            if self._should_exclude_image(abs_url, img):
                continue
            
            if self.config.debug and not is_pre_clean:
                print(f"      [DEBUG] âœ… ä¿ç•™åœ–ç‰‡: {abs_url[:80]}...")
            
            images.append({
                'url': abs_url,
                'alt': img.get('alt', ''),
                'title': img.get('title', ''),
                'width': img.get('width'),
                'height': img.get('height')
            })
        
        return images
    
    def _get_image_src(self, img) -> Optional[str]:
        """å¾å…ƒç´ ä¸­æå–åœ–ç‰‡ URLï¼ˆæ”¯æ´å¤šç¨® lazy-load æ–¹æ¡ˆï¼‰"""
        # å„ªå…ˆé †åºï¼šdata å±¬æ€§é€šå¸¸åŒ…å«é«˜è§£æåº¦åœ–ç‰‡
        src_attrs = [
            'data-src',
            'data-lazy-src', 
            'data-original',
            'data-lazy',
            'data-full-url',
            'data-image',
            'data-bg',
            'data-background',
            'data-hi-res-src',
            'data-large-file',
            'src',  # åŸå§‹ src æ”¾å¾Œé¢ï¼Œå› ç‚ºå¯èƒ½æ˜¯ placeholder
        ]
        
        for attr in src_attrs:
            value = img.get(attr)
            if value and not value.startswith('data:') and not 'placeholder' in value.lower():
                return value
        
        # è™•ç† srcset
        srcset = img.get('srcset') or img.get('data-srcset')
        if srcset:
            # å–æœ€å¤§çš„é‚£å¼µï¼ˆé€šå¸¸åœ¨æœ€å¾Œï¼‰
            parts = [p.strip().split()[0] for p in srcset.split(',') if p.strip()]
            if parts:
                return parts[-1]  # å–æœ€å¾Œä¸€å€‹ï¼ˆé€šå¸¸æœ€å¤§ï¼‰
        
        # æœ€å¾Œæª¢æŸ¥ srcï¼ˆå¯èƒ½æ˜¯å”¯ä¸€ä¾†æºï¼‰
        src = img.get('src')
        if src and not src.startswith('data:'):
            return src
        
        return None
    
    def _merge_images(self, main_images: list[dict], all_images: list[dict]) -> list[dict]:
        """åˆä½µåœ–ç‰‡åˆ—è¡¨ï¼Œå»é‡"""
        seen_urls = set()
        merged = []
        
        # å„ªå…ˆåŠ å…¥ä¸»å…§å®¹å€å¡Šçš„åœ–ç‰‡
        for img in main_images:
            if img['url'] not in seen_urls:
                seen_urls.add(img['url'])
                merged.append(img)
        
        # è£œä¸Šå¯èƒ½æ¼æ‰çš„åœ–ç‰‡ï¼ˆä½†è¦æ›´åš´æ ¼éæ¿¾ï¼‰
        for img in all_images:
            if img['url'] not in seen_urls:
                # åªè£œä¸Šçœ‹èµ·ä¾†åƒå…§å®¹åœ–ç‰‡çš„ï¼ˆæœ‰ alt æˆ–åœ¨ç‰¹å®šè·¯å¾‘ï¼‰
                url_lower = img['url'].lower()
                has_alt = bool(img.get('alt', '').strip())
                looks_like_content = any(kw in url_lower for kw in [
                    '/uploads/', '/images/', '/media/', '/content/',
                    '/wp-content/', '/assets/img', '/photos/', '/gallery/'
                ])
                
                if has_alt or looks_like_content:
                    seen_urls.add(img['url'])
                    merged.append(img)
                    if self.config.debug:
                        print(f"      [DEBUG] è£œå›åœ–ç‰‡: {img['url'][:60]}...")
        
        return merged
    
    def _should_exclude_image(self, url: str, img_tag) -> bool:
        """åˆ¤æ–·åœ–ç‰‡æ˜¯å¦æ‡‰è©²æ’é™¤ï¼ˆæ›´å¯¬é¬†çš„ç­–ç•¥ï¼‰"""
        url_lower = url.lower()
        
        # è·³é data URI
        if url_lower.startswith('data:'):
            if self.config.debug:
                print(f"      [DEBUG] â­ï¸  æ’é™¤ data URI")
            return True
        
        # è·³éæ˜é¡¯çš„è¿½è¹¤åƒç´ 
        if 'pixel' in url_lower and ('gif' in url_lower or '1x1' in url_lower):
            if self.config.debug:
                print(f"      [DEBUG] â­ï¸  æ’é™¤è¿½è¹¤åƒç´ : {url[:60]}...")
            return True
        
        # æª¢æŸ¥ URL patternï¼ˆä½¿ç”¨æ›´ç²¾ç¢ºçš„æ­£å‰‡ï¼‰
        for pattern in self.config.excluded_image_patterns:
            if re.search(pattern, url_lower):
                if self.config.debug:
                    print(f"      [DEBUG] â­ï¸  æ’é™¤ï¼ˆç¬¦åˆ pattern: {pattern}ï¼‰: {url[:60]}...")
                return True
        
        # æª¢æŸ¥å°ºå¯¸ï¼ˆåªæœ‰åœ¨æ˜ç¢ºæŒ‡å®šä¸”éå¸¸å°æ™‚æ‰æ’é™¤ï¼‰
        width = img_tag.get('width')
        height = img_tag.get('height')
        
        if width and height:
            try:
                w = int(re.sub(r'\D', '', str(width)))
                h = int(re.sub(r'\D', '', str(height)))
                # åªæ’é™¤éå¸¸å°çš„åœ–ç‰‡ï¼ˆå¦‚ 1x1 è¿½è¹¤åƒç´ ï¼‰
                if w > 0 and h > 0 and w <= 10 and h <= 10:
                    if self.config.debug:
                        print(f"      [DEBUG] â­ï¸  æ’é™¤ï¼ˆå°ºå¯¸ {w}x{h} å¤ªå°ï¼‰: {url[:60]}...")
                    return True
            except ValueError:
                pass
        
        return False
    
    def _to_markdown(self, soup: BeautifulSoup, base_url: str) -> str:
        """å°‡ HTML è½‰æ›ç‚º Markdown"""
        lines = []
        
        for element in soup.descendants:
            if element.name is None:
                continue
            
            text = element.get_text(strip=True) if hasattr(element, 'get_text') else ''
            
            if element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                level = int(element.name[1])
                if text:
                    lines.append(f"\n{'#' * level} {text}\n")
            
            elif element.name == 'p':
                if text and not any(text in line for line in lines[-3:] if lines):
                    lines.append(f"\n{text}\n")
            
            elif element.name == 'li':
                if text and not any(text in line for line in lines[-3:] if lines):
                    lines.append(f"- {text}")
            
            elif element.name == 'img':
                src = self._get_image_src(element)
                alt = element.get('alt', '')
                if src:
                    abs_url = urljoin(base_url, src)
                    lines.append(f"\n![{alt}]({abs_url})\n")
            
            elif element.name == 'a':
                href = element.get('href', '')
                if href and text and not href.startswith('#'):
                    abs_url = urljoin(base_url, href)
                    pass
            
            elif element.name == 'blockquote':
                if text:
                    lines.append(f"\n> {text}\n")
            
            elif element.name == 'pre' or element.name == 'code':
                if text and element.parent.name != 'pre':
                    lines.append(f"`{text}`")
                elif text:
                    lines.append(f"\n```\n{text}\n```\n")
        
        # æ¸…ç†çµæœ
        markdown = '\n'.join(lines)
        markdown = re.sub(r'\n{3,}', '\n\n', markdown)
        markdown = markdown.strip()
        
        return markdown


# ============================================
# æª”æ¡ˆç”¢ç”Ÿå™¨ (File Generator)
# ============================================

class FileGenerator:
    """è² è²¬ç”¢ç”Ÿè¼¸å‡ºæª”æ¡ˆ"""
    
    def __init__(self, config: CrawlerConfig):
        self.config = config
    
    def generate_page_yaml(self, url: str, metadata: dict, page_path: Path) -> dict:
        """ç”¢ç”Ÿé é¢ YAML è¨­å®š"""
        parsed = urlparse(url)
        
        return {
            '_generated': {
                'tool': 'website-crawler',
                'version': '1.2.0',
                'crawled_at': format_datetime()
            },
            'seo': {
                'meta_title': metadata.get('title', ''),
                'meta_description': metadata.get('description', ''),
                'meta_keywords': metadata.get('keywords', []),
                'canonical_url': metadata.get('canonical') or url,
                'robots': 'index, follow'
            },
            'url': {
                'original': url,
                'current': url,
                'path': parsed.path or '/',
                'slug': page_path.name or 'index',
                'redirects': []
            },
            'schema': {
                '@context': 'https://schema.org',
                '@type': 'WebPage',
                'name': metadata.get('title', ''),
                'description': metadata.get('description', ''),
                'url': url,
                'isPartOf': {
                    '@type': 'WebSite',
                    'name': parsed.netloc,
                    'url': f"{parsed.scheme}://{parsed.netloc}"
                },
                'breadcrumb': self._generate_breadcrumb(url)
            },
            'open_graph': {
                'og_type': 'website',
                'og_title': metadata.get('og_title') or metadata.get('title', ''),
                'og_description': metadata.get('og_description') or metadata.get('description', ''),
                'og_url': url,
                'og_site_name': parsed.netloc,
                'og_locale': 'zh_TW',
                'og_image': metadata.get('og_image', ''),
                'og_image_width': 1200,
                'og_image_height': 630,
                'og_image_alt': ''
            },
            'twitter': {
                'card': 'summary_large_image',
                'title': metadata.get('title', ''),
                'description': metadata.get('description', ''),
                'image': metadata.get('og_image', '')
            },
            'metadata': {
                'page_type': 'general',
                'language': 'zh-TW',
                'last_modified': format_datetime(),
                'crawled_at': format_datetime(),
                'content_status': 'draft'
            }
        }
    
    def _generate_breadcrumb(self, url: str) -> dict:
        """ç”¢ç”ŸéºµåŒ…å±‘çµæ§‹"""
        parsed = urlparse(url)
        path_parts = [p for p in parsed.path.split('/') if p]
        
        items = [{
            '@type': 'ListItem',
            'position': 1,
            'name': 'é¦–é ',
            'item': f"{parsed.scheme}://{parsed.netloc}"
        }]
        
        current_path = ''
        for i, part in enumerate(path_parts):
            current_path += f"/{part}"
            items.append({
                '@type': 'ListItem',
                'position': i + 2,
                'name': unquote(part).replace('-', ' ').title(),
                'item': f"{parsed.scheme}://{parsed.netloc}{current_path}"
            })
        
        return {
            '@type': 'BreadcrumbList',
            'itemListElement': items
        }
    
    def generate_image_yaml(self, image_info: dict, local_path: str) -> dict:
        """ç”¢ç”Ÿåœ–ç‰‡æè¿° YAML"""
        filename = os.path.basename(local_path)
        name, ext = os.path.splitext(filename)
        
        return {
            '_generated': {
                'tool': 'website-crawler',
                'version': '1.2.0',
                'crawled_at': format_datetime()
            },
            'filename': filename,
            'format': ext.lstrip('.').lower(),
            'description': {
                'alt_text': image_info.get('alt', ''),
                'detailed': 'ï¼ˆå¾… AI åˆ†æç”¢ç”Ÿè©³ç´°æè¿°ï¼‰',
                'text_content': None,
                'main_elements': [],
                'mood': '',
                'suggested_usage': []
            },
            'seo': {
                'suggested_filename': filename,
                'keywords': []
            },
            'source': {
                'original_url': image_info.get('url', ''),
                'original_filename': os.path.basename(urlparse(image_info.get('url', '')).path),
                'page_url': image_info.get('page_url', ''),
                'crawled_at': format_datetime()
            },
            'copyright': {
                'status': 'unknown',
                'attribution': None,
                'license': None
            }
        }
    
    def generate_markdown(self, url: str, metadata: dict, content: str) -> str:
        """ç”¢ç”Ÿé é¢ Markdown"""
        frontmatter = {
            'source_url': url,
            'crawled_at': format_datetime()
        }
        
        yaml_str = yaml.dump(frontmatter, allow_unicode=True, default_flow_style=False)
        
        title = metadata.get('title', 'Untitled')
        
        return f"""---
{yaml_str.strip()}
---

# {title}

{content}
"""


# ============================================
# ä¸»çˆ¬èŸ²é¡åˆ¥ (Main Crawler)
# ============================================

class WebsiteCrawler:
    """ç¶²ç«™çˆ¬èŸ²ä¸»é¡åˆ¥"""
    
    def __init__(self, config: Optional[CrawlerConfig] = None):
        self.config = config or CrawlerConfig()
        self.fetcher = WebFetcher(self.config)
        self.parser = ContentParser(self.config)
        self.generator = FileGenerator(self.config)
        
        self.crawled_urls = set()
        self.failed_urls = []
        self.image_hashes = {}
        self.all_urls = []
        
        self.stats = {
            'pages_found': 0,
            'pages_crawled': 0,
            'pages_failed': 0,
            'images_found': 0,
            'images_downloaded': 0,
            'images_excluded': 0,
            'images_duplicates': 0
        }
    
    def crawl(self, target_url: str) -> dict:
        """åŸ·è¡Œçˆ¬èŸ²"""
        if not target_url.startswith(('http://', 'https://')):
            target_url = f"https://{target_url}"
        
        parsed = urlparse(target_url)
        domain = parsed.netloc
        
        print(f"\n{'='*60}")
        print(f"ğŸ•·ï¸  ç¶²ç«™å…§å®¹çˆ¬èŸ² v1.2.0")
        print(f"{'='*60}")
        print(f"ğŸ“ ç›®æ¨™ç¶²ç«™: {domain}")
        print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {self.config.output_dir}/{domain}")
        print(f"ğŸ”§ åŸ·è¡Œæ¨¡å¼: {self.config.mode}")
        if self.config.debug:
            print(f"ğŸ› Debug æ¨¡å¼: é–‹å•Ÿ")
        print(f"{'='*60}\n")
        
        output_base = Path(self.config.output_dir) / domain
        output_base.mkdir(parents=True, exist_ok=True)
        
        if self.config.mode == 'preview':
            return self._crawl_preview(target_url, domain, output_base)
        elif self.config.mode == 'continue':
            return self._crawl_continue(target_url, domain, output_base)
        else:
            return self._crawl_full(target_url, domain, output_base)
    
    def _crawl_preview(self, target_url: str, domain: str, output_base: Path) -> dict:
        """é è¦½æ¨¡å¼ï¼šåªçˆ¬é¦–é """
        parsed = urlparse(target_url)
        
        print("ğŸ“‹ æª¢æŸ¥ robots.txt...")
        robots = self.fetcher.fetch_robots_txt(domain)
        if robots:
            robots_path = output_base / 'robots.txt'
            robots_path.write_text(robots, encoding='utf-8')
            print(f"   âœ… å·²å„²å­˜ robots.txt")
        else:
            print(f"   âš ï¸  æ‰¾ä¸åˆ° robots.txt")
        
        print("\nğŸ” å–å¾—ç¶²ç«™é é¢åˆ—è¡¨...")
        self.all_urls = self._discover_urls(domain, target_url)
        self.stats['pages_found'] = len(self.all_urls)
        print(f"   ğŸ“„ ç™¼ç¾ {len(self.all_urls)} å€‹é é¢")
        
        urls_file = output_base / '_pending_urls.json'
        with open(urls_file, 'w', encoding='utf-8') as f:
            json.dump({
                'target_url': target_url,
                'domain': domain,
                'urls': self.all_urls,
                'crawled': []
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\n{'='*60}")
        print("ğŸ  é è¦½æ¨¡å¼ï¼šçˆ¬å–é¦–é ...")
        print(f"{'='*60}\n")
        
        homepage_url = f"{parsed.scheme}://{parsed.netloc}"
        self._crawl_page(homepage_url, domain, output_base)
        
        preview_report = self._generate_preview_report(domain, output_base)
        report_path = output_base / '_preview_report.yml'
        with open(report_path, 'w', encoding='utf-8') as f:
            yaml.dump(preview_report, f, allow_unicode=True, default_flow_style=False)
        
        print(f"\n{'='*60}")
        print("ğŸ‘€ é¦–é é è¦½å®Œæˆ")
        print(f"{'='*60}")
        print(f"\nğŸ“„ é è¦½æª”æ¡ˆä½ç½®:")
        print(f"   â€¢ Markdown: {output_base}/index.md")
        print(f"   â€¢ YAML è¨­å®š: {output_base}/index.yml")
        images_dir = output_base / self.config.images_dir_name
        if images_dir.exists():
            img_count = len(list(images_dir.glob('*.*'))) // 2  # é™¤ä»¥2å› ç‚ºæœ‰yml
            print(f"   â€¢ åœ–ç‰‡: {images_dir}/ ({img_count} å¼µ)")
        
        print(f"\nğŸ“Š ç¶²ç«™æ¦‚æ³:")
        print(f"   â€¢ ç¸½å…±ç™¼ç¾ {len(self.all_urls)} å€‹é é¢å¾…çˆ¬å–")
        print(f"   â€¢ é¦–é åœ–ç‰‡: {self.stats['images_downloaded']} å¼µä¸‹è¼‰ / {self.stats['images_excluded']} å¼µæ’é™¤")
        
        print(f"\n{'='*60}")
        print("â¸ï¸  è«‹æª¢æŸ¥ä»¥ä¸Šé è¦½çµæœæ˜¯å¦ç¬¦åˆé æœŸ")
        print(f"{'='*60}")
        print(f"\nè‹¥ç¬¦åˆé æœŸï¼ŒåŸ·è¡Œä»¥ä¸‹æŒ‡ä»¤ç¹¼çºŒçˆ¬å–å…¨ç«™ï¼š")
        print(f"   python crawler.py {target_url} --continue")
        print(f"\nè‹¥éœ€è¦èª¿æ•´ï¼Œè«‹å‘ŠçŸ¥ä»¥ä¸‹è³‡è¨Šï¼š")
        print(f"   â€¢ å…§å®¹æ“·å–æ˜¯å¦æ­£ç¢ºï¼Ÿ")
        print(f"   â€¢ åœ–ç‰‡éæ¿¾æ˜¯å¦æ­£ç¢ºï¼Ÿ")
        print(f"   â€¢ Markdown æ ¼å¼æ˜¯å¦æ­£ç¢ºï¼Ÿ")
        print(f"{'='*60}\n")
        
        return preview_report
    
    def _crawl_continue(self, target_url: str, domain: str, output_base: Path) -> dict:
        """ç¹¼çºŒæ¨¡å¼"""
        urls_file = output_base / '_pending_urls.json'
        if not urls_file.exists():
            print("âŒ æ‰¾ä¸åˆ°ä¹‹å‰çš„çˆ¬å–ç‹€æ…‹ï¼Œè«‹å…ˆåŸ·è¡Œé è¦½æ¨¡å¼")
            print(f"   python crawler.py {target_url} --preview")
            return {}
        
        with open(urls_file, 'r', encoding='utf-8') as f:
            state = json.load(f)
        
        self.all_urls = state['urls']
        crawled = set(state.get('crawled', []))
        pending_urls = [u for u in self.all_urls if u not in crawled]
        
        self.stats['pages_found'] = len(self.all_urls)
        
        print(f"\nğŸ“‚ è¼‰å…¥ä¹‹å‰çš„çˆ¬å–ç‹€æ…‹")
        print(f"   â€¢ ç¸½é é¢: {len(self.all_urls)}")
        print(f"   â€¢ å·²çˆ¬å–: {len(crawled)}")
        print(f"   â€¢ å¾…çˆ¬å–: {len(pending_urls)}")
        
        print(f"\n{'='*60}")
        print("ğŸš€ ç¹¼çºŒçˆ¬å–å‰©é¤˜é é¢...")
        print(f"{'='*60}\n")
        
        for i, url in enumerate(pending_urls, 1):
            print(f"[{i}/{len(pending_urls)}] è™•ç†: {url}")
            self._crawl_page(url, domain, output_base)
            
            crawled.add(url)
            state['crawled'] = list(crawled)
            with open(urls_file, 'w', encoding='utf-8') as f:
                json.dump(state, f, ensure_ascii=False, indent=2)
            
            print()
        
        report = self._generate_report(domain)
        report_path = output_base / 'crawl-report.yml'
        with open(report_path, 'w', encoding='utf-8') as f:
            yaml.dump(report, f, allow_unicode=True, default_flow_style=False)
        
        urls_file.unlink(missing_ok=True)
        preview_report = output_base / '_preview_report.yml'
        preview_report.unlink(missing_ok=True)
        
        self._print_final_summary(output_base, report_path)
        
        return report
    
    def _crawl_full(self, target_url: str, domain: str, output_base: Path) -> dict:
        """å®Œæ•´æ¨¡å¼"""
        print("ğŸ“‹ æª¢æŸ¥ robots.txt...")
        robots = self.fetcher.fetch_robots_txt(domain)
        if robots:
            robots_path = output_base / 'robots.txt'
            robots_path.write_text(robots, encoding='utf-8')
            print(f"   âœ… å·²å„²å­˜ robots.txt")
            self._parse_robots(robots)
        else:
            print(f"   âš ï¸  æ‰¾ä¸åˆ° robots.txt")
        
        print("\nğŸ” å–å¾—ç¶²ç«™é é¢åˆ—è¡¨...")
        urls = self._discover_urls(domain, target_url)
        self.stats['pages_found'] = len(urls)
        print(f"   ğŸ“„ ç™¼ç¾ {len(urls)} å€‹é é¢")
        
        if not urls:
            print("   âš ï¸  æ‰¾ä¸åˆ°ä»»ä½•é é¢ï¼Œå˜—è©¦åªçˆ¬å–é¦–é ...")
            urls = [target_url]
        
        print(f"\n{'='*60}")
        print("ğŸš€ é–‹å§‹çˆ¬å–é é¢å…§å®¹...")
        print(f"{'='*60}\n")
        
        for i, url in enumerate(urls, 1):
            print(f"[{i}/{len(urls)}] è™•ç†: {url}")
            self._crawl_page(url, domain, output_base)
            print()
        
        report = self._generate_report(domain)
        report_path = output_base / 'crawl-report.yml'
        with open(report_path, 'w', encoding='utf-8') as f:
            yaml.dump(report, f, allow_unicode=True, default_flow_style=False)
        
        self._print_final_summary(output_base, report_path)
        
        return report
    
    def _print_final_summary(self, output_base: Path, report_path: Path):
        """è¼¸å‡ºæœ€çµ‚æ‘˜è¦"""
        print(f"\n{'='*60}")
        print("ğŸ“Š çˆ¬å–å®Œæˆæ‘˜è¦")
        print(f"{'='*60}")
        print(f"   ğŸ“„ é é¢: {self.stats['pages_crawled']}/{self.stats['pages_found']} æˆåŠŸ")
        print(f"   ğŸ–¼ï¸  åœ–ç‰‡: {self.stats['images_downloaded']} ä¸‹è¼‰ / {self.stats['images_excluded']} æ’é™¤ / {self.stats['images_duplicates']} é‡è¤‡")
        print(f"   ğŸ“ è¼¸å‡º: {output_base}")
        print(f"   ğŸ“‹ å ±å‘Š: {report_path}")
        print(f"{'='*60}\n")
    
    def _generate_preview_report(self, domain: str, output_base: Path) -> dict:
        """ç”¢ç”Ÿé è¦½å ±å‘Š"""
        return {
            'preview_report': {
                'status': 'pending_confirmation',
                'target_domain': domain,
                'preview_completed': format_datetime(),
                'homepage': {
                    'url': f"https://{domain}",
                    'markdown_file': str(output_base / 'index.md'),
                    'yaml_file': str(output_base / 'index.yml'),
                    'images_downloaded': self.stats['images_downloaded'],
                    'images_excluded': self.stats['images_excluded']
                },
                'site_overview': {
                    'total_pages_found': len(self.all_urls),
                    'pages_to_crawl': len(self.all_urls) - 1
                },
                'next_steps': {
                    'if_satisfied': f"python crawler.py https://{domain} --continue",
                    'if_not_satisfied': "è«‹å‘ŠçŸ¥éœ€è¦èª¿æ•´çš„åœ°æ–¹"
                }
            }
        }
    
    def _parse_robots(self, robots_txt: str):
        """è§£æ robots.txt"""
        disallowed = []
        for line in robots_txt.split('\n'):
            line = line.strip().lower()
            if line.startswith('disallow:'):
                path = line.replace('disallow:', '').strip()
                if path:
                    disallowed.append(path)
        
        if disallowed:
            print(f"   â„¹ï¸  ç™¼ç¾ {len(disallowed)} å€‹ç¦æ­¢è·¯å¾‘")
    
    def _discover_urls(self, domain: str, start_url: str) -> list[str]:
        """ç™¼ç¾ç¶²ç«™æ‰€æœ‰ URL"""
        urls = set()

        print("   å˜—è©¦ sitemap.xml...")
        sitemap_urls = self.fetcher.fetch_sitemap(domain)
        if sitemap_urls:
            print(f"   âœ… å¾ sitemap å–å¾— {len(sitemap_urls)} å€‹ URL")
            # æ­£è¦åŒ– sitemap URLs
            urls.update(normalize_url(u) for u in sitemap_urls)

        print("   æƒæé¦–é é€£çµ...")
        html = self.fetcher.fetch(start_url)
        if html:
            soup = BeautifulSoup(html, 'html.parser')
            for a in soup.find_all('a', href=True):
                href = a['href']
                abs_url = urljoin(start_url, href)
                parsed = urlparse(abs_url)

                if parsed.netloc == domain:
                    # æ­£è¦åŒ– URLï¼ˆç§»é™¤çµå°¾æ–œç·šï¼‰
                    clean_url = normalize_url(f"{parsed.scheme}://{parsed.netloc}{parsed.path}")
                    urls.add(clean_url)

        # æ­£è¦åŒ– start_url
        urls.add(normalize_url(start_url))

        # éæ¿¾æ’é™¤è·¯å¾‘
        if self.config.excluded_paths:
            filtered_urls = set()
            for url in urls:
                path = urlparse(url).path.strip('/')
                # æª¢æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•æ’é™¤è·¯å¾‘
                excluded = False
                for excluded_path in self.config.excluded_paths:
                    if path == excluded_path or path.startswith(f"{excluded_path}/"):
                        excluded = True
                        break
                if not excluded:
                    filtered_urls.add(url)

            excluded_count = len(urls) - len(filtered_urls)
            if excluded_count > 0:
                print(f"   â­ï¸  éæ¿¾ {excluded_count} å€‹æ’é™¤è·¯å¾‘")
            urls = filtered_urls

        urls = sorted(list(urls))

        return urls

    def _crawl_page(self, url: str, domain: str, output_base: Path):
        """çˆ¬å–å–®ä¸€é é¢"""
        # æ­£è¦åŒ– URL ä»¥é¿å…é‡è¤‡çˆ¬å–
        url = normalize_url(url)

        if url in self.crawled_urls:
            print("   â­ï¸  å·²è™•ç†éï¼Œè·³é")
            return

        self.crawled_urls.add(url)
        
        html = self.fetcher.fetch(url)
        if not html:
            print("   âŒ ç„¡æ³•å–å¾—é é¢å…§å®¹")
            self.failed_urls.append({'url': url, 'error': 'Fetch failed'})
            self.stats['pages_failed'] += 1
            return
        
        try:
            parsed = self.parser.parse(html, url)
        except Exception as e:
            print(f"   âŒ è§£æå¤±æ•—: {e}")
            self.failed_urls.append({'url': url, 'error': str(e)})
            self.stats['pages_failed'] += 1
            return
        
        parsed_url = urlparse(url)
        url_path = parsed_url.path.strip('/')
        
        if not url_path:
            page_dir = output_base
        else:
            url_path = re.sub(r'\.(html?|php|aspx?)$', '', url_path, flags=re.IGNORECASE)
            page_dir = output_base / url_path
        
        page_dir.mkdir(parents=True, exist_ok=True)

        images_dir = page_dir / self.config.images_dir_name
        downloaded_images = []
        
        print(f"   ğŸ” ç™¼ç¾ {len(parsed['images'])} å¼µåœ–ç‰‡")
        
        if parsed['images']:
            images_dir.mkdir(exist_ok=True)
            self.stats['images_found'] += len(parsed['images'])
            
            for img in parsed['images']:
                result = self._download_image(img, images_dir, url)
                if result:
                    downloaded_images.append(result)
        else:
            print(f"   âš ï¸  æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„åœ–ç‰‡")
        
        markdown = parsed['markdown']
        for img in downloaded_images:
            old_url = img['original_url']
            new_path = f"./{self.config.images_dir_name}/{img['filename']}"
            markdown = markdown.replace(old_url, new_path)
        
        md_content = self.generator.generate_markdown(
            url, 
            parsed['metadata'], 
            markdown
        )
        md_path = page_dir / 'index.md'
        md_path.write_text(md_content, encoding='utf-8')
        print(f"   ğŸ“ å·²ç”¢ç”Ÿ: {md_path.relative_to(output_base)}" if page_dir != output_base else f"   ğŸ“ å·²ç”¢ç”Ÿ: index.md")
        
        yaml_content = self.generator.generate_page_yaml(
            url,
            parsed['metadata'],
            page_dir.relative_to(output_base) if page_dir != output_base else Path('.')
        )
        yml_path = page_dir / 'index.yml'
        with open(yml_path, 'w', encoding='utf-8') as f:
            yaml.dump(yaml_content, f, allow_unicode=True, default_flow_style=False)
        print(f"   ğŸ“‹ å·²ç”¢ç”Ÿ: {yml_path.relative_to(output_base)}" if page_dir != output_base else f"   ğŸ“‹ å·²ç”¢ç”Ÿ: index.yml")
        
        self.stats['pages_crawled'] += 1
    
    def _download_image(self, image_info: dict, images_dir: Path, page_url: str) -> Optional[dict]:
        """ä¸‹è¼‰å–®å¼µåœ–ç‰‡"""
        url = image_info['url']
        
        content = self.fetcher.fetch(url, binary=True)
        if not content:
            print(f"   âš ï¸  ç„¡æ³•ä¸‹è¼‰åœ–ç‰‡: {url[:60]}...")
            return None
        
        img_hash = get_file_hash(content)
        if img_hash in self.image_hashes:
            print(f"   ğŸ”„ é‡è¤‡åœ–ç‰‡ï¼Œè·³é: {url[:60]}...")
            self.stats['images_duplicates'] += 1
            return None
        
        parsed_url = urlparse(url)
        original_name = os.path.basename(parsed_url.path)
        if not original_name or '.' not in original_name:
            ext = mimetypes.guess_extension('image/jpeg') or '.jpg'
            original_name = f"image-{img_hash[:8]}{ext}"
        
        filename = sanitize_filename(slugify(os.path.splitext(original_name)[0]) + os.path.splitext(original_name)[1])
        
        filepath = images_dir / filename
        counter = 1
        while filepath.exists():
            name, ext = os.path.splitext(filename)
            filepath = images_dir / f"{name}-{counter}{ext}"
            counter += 1
        
        filepath.write_bytes(content)
        self.image_hashes[img_hash] = str(filepath)
        
        image_info['page_url'] = page_url
        img_yaml = self.generator.generate_image_yaml(image_info, str(filepath))
        yaml_path = filepath.with_suffix('.yml')
        with open(yaml_path, 'w', encoding='utf-8') as f:
            yaml.dump(img_yaml, f, allow_unicode=True, default_flow_style=False)
        
        print(f"   ğŸ–¼ï¸  å·²ä¸‹è¼‰: {filepath.name}")
        self.stats['images_downloaded'] += 1
        
        return {
            'original_url': url,
            'filename': filepath.name,
            'local_path': str(filepath)
        }
    
    def _generate_report(self, domain: str) -> dict:
        """ç”¢ç”Ÿçˆ¬å–å ±å‘Š"""
        return {
            'crawl_report': {
                'target_domain': domain,
                'crawl_completed': format_datetime(),
                'pages': {
                    'total_found': self.stats['pages_found'],
                    'successfully_crawled': self.stats['pages_crawled'],
                    'failed': self.stats['pages_failed'],
                    'failed_urls': self.failed_urls
                },
                'images': {
                    'total_found': self.stats['images_found'],
                    'downloaded': self.stats['images_downloaded'],
                    'excluded': self.stats['images_excluded'],
                    'duplicates': self.stats['images_duplicates']
                },
                'files_generated': {
                    'markdown_files': self.stats['pages_crawled'],
                    'yaml_config_files': self.stats['pages_crawled'],
                    'image_description_files': self.stats['images_downloaded']
                }
            }
        }


# ============================================
# CLI ä»‹é¢
# ============================================

def main():
    parser = argparse.ArgumentParser(
        description='ç¶²ç«™å…§å®¹çˆ¬èŸ²èˆ‡å‚™ä»½å·¥å…· v1.2.0',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
åŸ·è¡Œæ¨¡å¼:
  é è¨­æ¨¡å¼    åªçˆ¬é¦–é ï¼Œç”¢ç”Ÿé è¦½ä¾›ç¢ºèªï¼ˆå®‰å…¨æ¨¡å¼ï¼‰
  --preview   åŒä¸Šï¼Œæ˜ç¢ºæŒ‡å®šé è¦½æ¨¡å¼
  --continue  ç¢ºèªé è¦½ç„¡èª¤å¾Œï¼Œç¹¼çºŒçˆ¬å–å…¨ç«™
  --full      è·³éç¢ºèªï¼Œç›´æ¥ä¸€æ¬¡çˆ¬å®Œï¼ˆæ…ç”¨ï¼‰
  --debug     é¡¯ç¤ºè©³ç´°çš„åœ–ç‰‡æ“·å–éç¨‹ï¼ˆè¨ºæ–·ç”¨ï¼‰

ç¯„ä¾‹:
  python crawler.py https://example.com              # é è¦½æ¨¡å¼
  python crawler.py https://example.com --debug      # é è¦½ + debug è¨Šæ¯
  python crawler.py https://example.com --continue   # ç¹¼çºŒçˆ¬å–å…¨ç«™
  python crawler.py https://example.com --full       # ç›´æ¥çˆ¬å…¨ç«™
        """
    )
    
    parser.add_argument(
        'url',
        nargs='?',
        help='ç›®æ¨™ç¶²ç«™ç¶²å€'
    )
    
    mode_group = parser.add_mutually_exclusive_group()
    mode_group.add_argument(
        '--preview',
        action='store_true',
        help='é è¦½æ¨¡å¼ï¼šåªçˆ¬é¦–é ä¾›ç¢ºèªï¼ˆé è¨­æ¨¡å¼ï¼‰'
    )
    mode_group.add_argument(
        '--continue',
        dest='continue_crawl',
        action='store_true',
        help='ç¹¼çºŒæ¨¡å¼ï¼šçˆ¬å–å‰©é¤˜é é¢'
    )
    mode_group.add_argument(
        '--full',
        action='store_true',
        help='å®Œæ•´æ¨¡å¼ï¼šç›´æ¥ä¸€æ¬¡çˆ¬å®Œ'
    )
    
    parser.add_argument(
        '--debug',
        action='store_true',
        help='Debug æ¨¡å¼ï¼šé¡¯ç¤ºè©³ç´°çš„åœ–ç‰‡æ“·å–éç¨‹'
    )
    
    parser.add_argument(
        '-o', '--output',
        default='../../../',  # ä¿®æ­£ï¼šè¼¸å‡ºåˆ°å°ˆæ¡ˆæ ¹ç›®éŒ„
        help='è¼¸å‡ºç›®éŒ„ (é è¨­: ../../../ å°ˆæ¡ˆæ ¹ç›®éŒ„)'
    )
    
    parser.add_argument(
        '-d', '--delay',
        type=float,
        default=2.0,
        help='è«‹æ±‚é–“éš”ç§’æ•¸ (é è¨­: 2.0)'
    )
    
    parser.add_argument(
        '-t', '--timeout',
        type=int,
        default=30,
        help='è«‹æ±‚è¶…æ™‚ç§’æ•¸ (é è¨­: 30)'
    )
    
    parser.add_argument(
        '-r', '--retries',
        type=int,
        default=3,
        help='æœ€å¤§é‡è©¦æ¬¡æ•¸ (é è¨­: 3)'
    )
    
    parser.add_argument(
        '--min-image-size',
        type=int,
        default=50,
        help='æœ€å°åœ–ç‰‡å°ºå¯¸ px (é è¨­: 50)'
    )

    parser.add_argument(
        '--exclude',
        type=str,
        default='',
        help='æ’é™¤çš„è·¯å¾‘ï¼ˆé€—è™Ÿåˆ†éš”ï¼‰ï¼Œä¾‹å¦‚: elementor-hf,category,hello-world'
    )

    parser.add_argument(
        '--format',
        type=str,
        choices=['default', 'pages'],
        default='default',
        help='è¼¸å‡ºæ ¼å¼ï¼šdefault (images/) æˆ– pages (assets/)'
    )

    args = parser.parse_args()
    
    if args.continue_crawl:
        mode = 'continue'
    elif args.full:
        mode = 'full'
    else:
        mode = 'preview'
        if not args.preview and not args.continue_crawl and not args.full:
            print("\nâš ï¸  æœªæŒ‡å®šåŸ·è¡Œæ¨¡å¼ï¼Œä½¿ç”¨é è¨­çš„ preview æ¨¡å¼")
            print("   â†’ åªæœƒçˆ¬å–é¦–é ä¾›æ‚¨ç¢ºèª")
            print("   â†’ ç¢ºèªç„¡èª¤å¾Œï¼Œè«‹åŸ·è¡Œ: --continue\n")
    
    target_url = args.url
    if not target_url:
        print("\nğŸ•·ï¸  ç¶²ç«™å…§å®¹çˆ¬èŸ²èˆ‡å‚™ä»½å·¥å…·")
        print("=" * 40)
        target_url = input("\nè«‹è¼¸å…¥è¦çˆ¬å–çš„ç¶²ç«™ç¶²å€: ").strip()
        
        if not target_url:
            print("âŒ æœªæä¾›ç¶²å€ï¼ŒçµæŸç¨‹å¼")
            sys.exit(1)
    
    # è§£ææ’é™¤è·¯å¾‘
    excluded_paths = [p.strip() for p in args.exclude.split(',') if p.strip()]

    # è¨­å®šåœ–ç‰‡ç›®éŒ„åç¨±
    images_dir_name = 'assets' if args.format == 'pages' else 'images'

    config = CrawlerConfig(
        mode=mode,
        debug=args.debug,
        output_dir=args.output,
        crawl_delay=args.delay,
        timeout=args.timeout,
        max_retries=args.retries,
        min_image_size=args.min_image_size,
        excluded_paths=excluded_paths,
        images_dir_name=images_dir_name
    )
    
    crawler = WebsiteCrawler(config)
    
    try:
        crawler.crawl(target_url)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ åŸ·è¡ŒéŒ¯èª¤: {e}")
        if args.debug:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()